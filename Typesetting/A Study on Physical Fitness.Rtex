\documentclass[letterpaper,11pt]{article}
\usepackage{natbib}
\bibliographystyle{unsrtnat}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage[margin=1in, letterpaper]{geometry}
\usepackage{cite}
\usepackage{accents}
\renewcommand{\vec}[1]{\underaccent{\tilde}{#1}}
\usepackage[final]{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue,
	filecolor=magenta,
	urlcolor=blue         
}

\begin{document}

\title{\textbf{A Study on Physical Fitness}}
\author{Sreejit Roy\\ Vaishnavi Jaiswal\\ Mousam Sinha}
\maketitle

%\begin{abstract}

%\end{abstract}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}

The dataset contains various measures of heart and pulse rates taken on men in a physical fitness course.
The following are defined:

\textbf{\textit{Y}} $ \coloneqq $ oxygen used to complete the given task.

\textbf{\textit{$ X_1 $}} $ \coloneqq $ age of the participant.

\textbf{\textit{$ X_2 $}} $ \coloneqq $ weight of the participant.

\textbf{\textit{$ X_3 $}} $ \coloneqq $ time to run a given distance on a treadmill.

\textbf{\textit{$ X_4 $}} $ \coloneqq $ measure of pulse-rate at rest.

\textbf{\textit{$ X_5 $}} $ \coloneqq $ average pulse-rate during the run.

\textbf{\textit{$ X_6 $}} $ \coloneqq $ maximum pulse-rate during the run.

For the purpose of this study, \textbf{\textit{Y}} is taken as the response variable; the remaining as explanatory.

<<echo = FALSE, results = FALSE, message = FALSE>>=
rm(list = ls())

setwd("/Users/roysreejit/OneDrive/Academics/ISI/Sem 1/Regression Techniques/Project") # nolint

fitdata <- read.csv("fitness1.csv")
fitdata <- fitdata[, c("oxy", "age", "weight", "runtime",
                       "restpulse", "runpulse", "maxpulse")
]
colnames(fitdata) <- c("Y", "X1", "X2", "X3", "X4", "X5", "X6")
fitdata1 <- fitdata
attach(fitdata)

library(MASS)
library(lattice)
library(car)
library(lmtest)
library(olsrr)
library(randtests)
library(ridge)
library(knitr)
library(leaps)
@ 

\clearpage

\section{Preliminary Analysis}
For the given dataset, the correlation matrix is as follows:

<<echo = FALSE>>=
kable(cor(fitdata1))
@ 
\begin{center}
   \textbf{(Table 2.1)}
\end{center}

\textbf{Remark:}
\begin{enumerate}
	\item There exists a high correlation between $ X_5 $ and $ X_6 $, viz., 0.93.
\end{enumerate}

Consider the plots of all pairs of columns as follows:

<<echo = FALSE, fig.height = 6, fig.width = 8>>=
pairs(fitdata1)
@ 

\begin{center}
   \textbf{(Figure 2.1)}
\end{center}

\textbf{Remarks:}
\begin{enumerate}
	\item There exists a high positive relationship between $ X_5 $ and $ X_6 $ 
	\item There is a linear relationship between $ Y $ and $ X_3 $ with a negative slope.
\end{enumerate}

\clearpage

\section{Model Fitting}

Primarily, consider the simple linear regression model
\begin{equation}
	\vec{Y} = \textbf{X}\vec{\beta} + \vec{\epsilon}
\end{equation}
where
$ \textbf{X}_{31\times 7} = (\vec{1},\ X_1,\ X_2,\ ...,\ X_6) $, $ \vec{\beta} = (\beta_0,\ \beta_1,\ ...,\ \beta_6)' $, $ \vec{\epsilon} = (\epsilon_1,\ \epsilon_2,\ ...,\ \epsilon_{31})' $;

with the assumptions:
\begin{itemize}
	\item $ \vec{\epsilon} \sim N(\vec{0}, \sigma^2\mathbf{I_{31}}) $
	\item \textbf{X} is non-stochastic and has full rank.
\end{itemize}

The following is a summary for the model considered.

<<>>=
fm <- lm(Y~ ., fitdata1)
summary(fm)
@ 

The following figure plots the response values (blue) and the predicted values (red).
It serves as a visual indication of the goodness of fit of the model.

<<echo = FALSE>>=
fm_pred <- predict(fm)
xyplot(fitdata1[, "Y"] + fm_pred ~ 1:31,
       type = c("g", "b"),
       ylab = "Response and Predicted Values",
       xlab = "Index",
       main = list("Full Regression Model"),
       auto.key = list(columns = 2,
                       text = c("Response", "Predicted")
       )
)
@ 

\begin{center}
   \textbf{(Figure 3.1)}
\end{center}

\clearpage

\section{Verification of Model Assumptions}

Along with model (1) (i.e., the full model), the following assumptions were made:

\begin{itemize}
	\item $ \vec{\epsilon} \sim N(\vec{0}, \sigma^2\mathbf{I_{31}}) $
	\item \textbf{X} is non-stochastic and has full rank.
\end{itemize}

In particular, the first assumption is that of homoscedasticity, normality,
and absense of autocorrelation. The second assumption is about independence of
explanatory variables and the non-stochastic nature of them.

In the following subsubsections we verify all the aforementioned assumptions except for
that of non-stochastic nature. We just assume that the explanatory variables are indeed
non-stochastic in nature.

\subsection{Homoscedasticity}
The assumption of homoscedasticity states that all the error terms have equal variance.

First, the residual plot is made for visually inspecting whether the assumption of homoscedasticity is violated or not:  

<<echo = FALSE, fig.height = 5, fig.width = 5>>=
ols_plot_resid_fit(fm)
@

\begin{center}
   \textbf{(Figure 4.1)}
\end{center}


\textbf{Remark:}
\begin{enumerate}
	\item There is no evident pattern in the residual plot, implying that the assumption of homoscedasticity is preserved.
\end{enumerate}

We now perform formal checks for violation of the assumption.
\subsubsection{Breusch–Pagan Test}
Breusch–Pagan test is performed for obtaining a formal (and more mathematical) conclusion about whether heteroscedasticity is present or not.

<<echo = FALSE>>=
bptest(fm)
@ 

\textbf{Remark:}
\begin{enumerate}
	\item p-value of 0.8302 means that there is no heteroscedasticity in the data-set.
\end{enumerate}

\subsection{Normality}
The assumption of normality states that all the errors are normally distributed.

The check the validy of the assumption, we first do the QQ-plot is as follows:

<<echo = FALSE, fig.height = 4.65, fig.width = 8>>=
ols_plot_resid_qq(fm)
@ 

\begin{center}
   \textbf{(Figure 4.2)}
\end{center}


\textbf{Remark:}
\begin{enumerate}
	\item Although most of the points are around the $ y = x $ line, it is to be noted
   that there is an oscillating pattern centred around the line. Proper mathematical tests
   are performed for a more concrete conclusion.
\end{enumerate}

\subsubsection{Shapiro-Wilk Test}
First, the Shapiro-Wilk test is performed for testing whether the assumption of normality is violated or not.

<<>>=
shapiro.test(residuals(fm))
@ 

\textbf{Remark:}
\begin{enumerate}
	\item p-value of 0.4603 indicates that the assumption of normality is not violated.
\end{enumerate}

\subsubsection{Kolmogorov-Smirnov Test}
Kolmogorov-Smirnov test is also performed just to ensure whether the conclusion agrees with that obtained using Shapiro-Wilk test or not.

<<>>=
ols_test_normality(fm)[1]
@ 

\textbf{Remark:}
\begin{enumerate}
	\item p-value of 0.563 implies that the assumption of normality is valid.
\end{enumerate}

\subsection{Autocorrelation}
Autocorrelation means that the errors are correlated with each other. It is assumed that autocorrelation is
not present in the model. For the verification, first the ACF of the residuals are plotted:

<<echo = FALSE>>=
acf(residuals(fm))
@ 

\begin{center}
   \textbf{(Figure 4.3)}
\end{center}


\textbf{Remark:}
\begin{enumerate}
	\item It is easy to observe that there exists no pattern denoting the presence of autocorrelation.
   I.e., there is no autocorrelation.
\end{enumerate}

Formal tests are performed to validate the claim.

\subsubsection{Durbin-Watson Test}
Durbin-Watson test is performed to check (mathematically) whether autocorrelation is present or not.

<<>>=
dwtest(fm)
@ 

\textbf{Remark:}
\begin{enumerate}
	\item From the test, we can conclude that there is no autocorrelation in model. Thus, the errors are independently distributed.
\end{enumerate}

\subsubsection{Runs Test}
Runs test is also performed as follows:
<<fig.height = 4.5, fig.width = 8>>=
runs.test(residuals(fm), plot = TRUE)
@ 

\begin{center}
   \textbf{(Figure 4.4)}
\end{center}


\textbf{Remark:}
\begin{enumerate}
	\item The test concludes that the residuals do not follow any pattern. This, in turn, implies that the residuals are not auto-correlated.
\end{enumerate}

\subsection{Collinearity}
A model is said to have multicollinearity when its regression matrix is not of full rank. In paricular, when some explanatory variables are
linearly dependent, multicollinearity occurs. It is assumed that there is no multicollinearity in model (1). To verify the claim, the following
are calculated.

\subsubsection{VIF}
First, the VIFs are checked for the model. As a rule of thumb, if  VIF value for an explanatory variable is higher that 5, we suspect that
multicollinearity is present.

<<>>=
vif(fm)
@ 

\textbf{Remark:}
\begin{enumerate}
   \item We see that $X_5$ and $X_6$ have high VIF values. Also table 2.1 and figure 2.1 indicate presence of high positive correlation among these two variables.
\end{enumerate}

\subsubsection{Condition Number}
The condition number is as follows:

<<>>=
kappa(fitdata1[, -c(1)])
@ 

\textbf{Remark:}
\begin{enumerate}
   \item On the basis of high condition number and VIF values for $X_5$ and $X_6$, we suspect that there is a presence of multicollinearity. Hence we perform remedies for multicollinearity in Section 6.
\end{enumerate}   


\clearpage

\section{Detection of Influential Points}

\subsection{Cook's D Plot}
We find points with high cook's distance.

<<echo = FALSE>>=
ols_plot_cooksd_bar(fm)
@ 

\begin{center}
   \textbf{(Figure 5.1)}
\end{center}


\textbf{Remark:}
\begin{enumerate}
   \item On the basis of above plot, we can see that observations 10, 15 and 20 are potential influential points.
\end{enumerate}

\subsection{DFFITS Plot}
This plot will help us to detect influential data points.

<<echo = FALSE>>=
ols_plot_dffits(fm)
@ 

\begin{center}
   \textbf{(Figure 5.2)}
\end{center}


\textbf{Remark:}
\begin{enumerate}
   \item On the basis of DFFITS plot, we can see that observations 10, 15 and 20 are potential influential points.
\end{enumerate}

\subsection{DFBETAS Plot}
This plot will help us to detect influential data points with respect to various regressor variables.

<<echo = FALSE>>=
ols_plot_dfbetas(fm)
@ 

\begin{center}
   \textbf{(Figure 5.3 and 5.4)}
\end{center}

\textbf{Remark:}
\begin{enumerate}
   \item On the basis of DFBETAS plot, we can see that observations 4, 10, 15, 17, and 20 are potential influential points.
\end{enumerate}

\subsection{Added Variable Plot}
This plot gives the relationship between response variable and a regressor variable when other variables are held constant.
Each added variable plot notes four different values, two of which are points with highest residuals, the others are those
with highest partial leverage.

<<>>=
avPlots(fm)
@ 

\begin{center}
   \textbf{(Figure 5.5)}
\end{center}

\textbf{Remark:}
\begin{enumerate}
   \item On the basis of above plots, we observe the $ 15^{th} $ and  $ 17^{th} $ points are definitely outliers.
\end{enumerate}

\subsection{Studentised Residual Plot}
We will try to find the outliers with the help of this plot. An observation is considered to be an outlier if the absolute value of studentised residual is greater than 2.

<<echo = FALSE>>=
ols_plot_resid_stud_fit(fm)
@ 

\begin{center}
   \textbf{(Figure 5.6)}
\end{center}

<<echo = FALSE>>=
outlierTest(fm)
@ 

\textbf{Remark:}
\begin{enumerate}
   \item On the basis of the above plot, we can see that observations 15 and 17 are outliers.
\end{enumerate}

\subsection{Hat Values}
We will calculate the hat diagonal values for all observations. Observations for which the hat diagonal is greater than $2p/n$ where $p$ is the number of regression parameters and $n$ is the number of observations.

<<echo = FALSE>>=
plot(hatvalues(fm), col = 4)
abline(h = 14 / 31, col = 2)
se2 <- seq(1, 31)
idc2 <- (hatvalues(fm) > 14 / 31)
text(se2[idc2], hatvalues(fm)[idc2],
labels = rownames(fitdata1)[idc2], cex = 0.6, pos = 4)
@ 

\begin{center}
   \textbf{(Figure 5.7)}
\end{center}

\textbf{Remark:}
\begin{enumerate}
   \item On the basis of the above plot, we can see that observations 10 is a high leverage point.
\end{enumerate}

\subsection{Residual vs Leverage Plot}

<<echo = FALSE>>=
plot(fm, which = 5)
@ 

\begin{center}
   \textbf{(Figure 5.8)}
\end{center}

\textbf{Remarks:}
\begin{enumerate}
   \item On the basis of the above plot, note that $15^{th}$ is an outlier; $ 10^{th} $ point is high leverage.
   \item We also note that $ 20^{th} $ is an influential point, mostly because it has moderately high residual as well as leverage.
\end{enumerate}

\subsection*{Conclusion}

\begin{itemize}
   \item From all the above subsubsections, we note that the following observations are outliers:
   $15^{th}$, $17^{th}$, and $20^{th}$. And the $10^{th}$ observation has high leverage.
\end{itemize}

\subsection*{Remedy}
We remove $15^{th}$, $17^{th}$, and $20^{th}$ observations from the dataset and fit the model (1).
We estimate the Y values for the $15^{th}$, $17^{th}$, and $20^{th}$ observations from this model.
Then we use the obtained Y values as observed values in the dataset. Then we use the newly formed
dataset for fitting model (1) again.

The following is the summary of the model (1) fitted with rectified Y vector.

<<echo = FALSE>>=
fitdata2 <- fitdata[-c(15, 17, 20), ]
fitdata$Y[c(15, 17, 20)] <- predict(lm(Y~ ., fitdata2),
fitdata1[c(15, 17, 20), ])

fm2 <- lm(Y~ ., fitdata)
summary(fm2)
@ 

The following tests are performed to check whether the assumptions are being satisfied or not.

<<>>=
bptest(fm2)
shapiro.test(residuals(fm2))
dwtest(fm2)
vif(fm2)
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item Heteroscedasticity, non-normality, and autocorrelation are not present.
   \item Multicollinearity still exists, which is obvious as multicollinearity mainly depends on the regression matrix.
\end{enumerate}

\clearpage

\section{Remedies For Multicollinearity}

\subsection{Ridge Regression}
Ridge regression is performed as a remedy for multicollinearity.

<<echo = FALSE>>=
rm <- linearRidge(fitdata, alpha = 0)
summary(rm)
rm_pred <- predict(rm)
xyplot(fitdata1[, "Y"] + rm_pred ~ 1:31,
       type = c("g", "b"),
       ylab = "Response and Predicted Values",
       xlab = "Index",
       main = list("Ridge Regression Model"),
       auto.key = list(columns = 2,
                       text = c("Response", "Predicted")
       )
)
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item This model does not have the problem of multicollinearity. This directly follows from the theory of ridge regression.
   \item In the following subsections, other methods of removing multicollinearity are explored in detail.
\end{enumerate}

\subsection{Removing Covariates}
Another possible way of removing multicollinearity is that of selecting subsets of explanatory variables.
It is already noted that the VIF values for $ X_5 $ and $ X_6 $ are high. It is also noted previously that
these two are correlated. Therefore, if one of those two is removed, the problem of multicollinearity may
be resolved. In the following two subsections, we remove each of those two (one at a time) and perform further
analysis to obtain a good model.

\subsubsection{Removing Runpulse}
We remove Run pulse ($ X_5 $) from the list of explanatory variables. We then fit a linear model using the remaining five explanatory variables. 
Tests are performed to ensure that the model satisfies all the model assumptions. 

<<echo = FALSE>>=
m1 <- lm(Y ~ X1 + X2 + X3 + X4 + X6, fitdata[, -c(6)])
summary(m1)

ols_plot_resid_fit(m1)
bptest(m1)

ols_plot_resid_qq(m1)
shapiro.test(residuals(m1))
ols_test_normality(m1)[1]

acf(residuals(m1))
dwtest(m1)
runs.test(residuals(m1), plot = TRUE)

vif(m1)
kappa(fitdata[, -c(1, 6)])
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item Note that the problem of multicollinearity is resolved in this model.
   \item Although it is to be noted that all explanatory variables are not significant.
\end{enumerate}

\subsubsection{Removing Maxpulse}
We remove Max pulse ($ X_6 $) from the list of explanatory variables. We then fit a linear model using the remaining five explanatory variables. 
Tests are performed to ensure that the model satisfies all the model assumptions. 

<<echo = FALSE>>=
m2 <- lm(Y ~ X1 + X2 + X3 + X4 + X5, fitdata[, -c(7)])
summary(m2)

ols_plot_resid_fit(m2)
bptest(m2)

ols_plot_resid_qq(m2)
shapiro.test(residuals(m2))
ols_test_normality(m2)[1]

acf(residuals(m2))
dwtest(m2)
@ 

<<echo = FALSE, fig.height = 6, fig.width = 8>>=
runs.test(residuals(m2), plot = TRUE)
@ 

<<>>=
vif(m2)
kappa(fitdata[, -c(1, 7)])
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item Note that the problem of multicollinearity is resolved in this model.
   \item Although it is to be noted that all explanatory variables are not significant.
\end{enumerate}

\clearpage

\section{Model Selection}

\subsection{Model 1}
Here, consider the model obtained in section (6.2.1). We consider the added variable plot for the considered model.

<<echo = FALSE>>=
avPlots(m1)
@ 

Note that the slope in each plot reflects the partial regression coefficients from the actual regression model. Therefore, a slope closer to zero would mean that the partial regression coefficient is closer to zero corresponding to that explanatory variable. Since partial regression coefficients measure the expected change in $ Y $ for one unit change in the corresponding explanatory variable, provided all the other explanatory variables are held constant, a slope closer to zero signifies that the corresponding explanatory does not have much of an effect in the model.

\textbf{Remark:}
\begin{enumerate}
   \item $ X_2 $ and $ X_4 $ are expected not to have highly significant effect on the response.
\end{enumerate}

\subsection*{Subset Selection}
The following code snippet chooses the best model for each possible number of explanatory variables.

<<>>=
regfit1 <- regsubsets(Y ~ X1 + X2 + X3 + X4 + X6, data = fitdata)
regsumm1 <- summary(regfit1)
regsumm1
@ 

The following table table compares the obtained models (5 models) interms of AIC, BIC, Adjusted R-squared, and Mallow's Cp.

<<echo = FALSE>>=
mat1 <- cbind(regsumm1$cp, regsumm1$adjr2, regsumm1$bic,
ols_step_all_possible(m1)$aic[c(1, 6, 16, 26, 31)])

rownames(mat1) <- c("X3", "X1, X3", "X1, X2, X3",
"X1, X2, X3, X6", "X1, X2, X3, X4, X6")
colnames(mat1) <- c("cp", "adjr2", "bic", "aic")
kable(mat1)
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item In terms of Mallow's cp and AIC, the fourth model seems to be the best.
   \item In terms of adjusted R-squared, model 5 is the better one.
   \item The third model is best in terms of BIC.
\end{enumerate}

We now use stepwise selection method to evaluate the best possible model as follows.

<<>>=
ols_step_both_aic(m1, details = TRUE)
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item The stepwise selection method chooses the model Y on X1, X2, X3, and X6.
   \item This model is preferable in terms of Mallow's cp and AIC also.
   \item The adjusted R-square for this model is 0.807, which is close to the model
   preferable in terms of adjusted R-squared criterion.
   \item Therefore, we select model 4 (i.e., Y on X1, X2, X3, and X6) among all possible
   subsets of model 1 (i.e., Y on X1, X2, X3, X4, and X6).
\end{enumerate}

\subsection{Model 2}
Here, consider the model obtained in section (6.2.2). We consider the added variable plot for the considered model.

<<echo = FALSE>>=
avPlots(m2)
@ 

\textbf{Remark:}
\begin{enumerate}
   \item $ X_2 $ and $ X_4 $ are expected not to have highly significant effect on the response.
\end{enumerate}

\subsection*{Subset Selection}
The following code snippet chooses the best model for each possible number of explanatory variables.

<<>>=
regfit2 <- regsubsets(Y ~ X1 + X2 + X3 + X4 + X5, data = fitdata)
regsumm2 <- summary(regfit2)
regsumm2
@ 

The following table table compares the obtained models (5 models) interms of AIC, BIC, Adjusted R-squared, and Mallow's Cp.

<<echo = FALSE>>=
mat2 <- cbind(regsumm2$cp, regsumm2$adjr2, regsumm2$bic,
ols_step_all_possible(m2)$aic[c(1, 6, 16, 26, 31)])

rownames(mat2) <- c("X3", "X1, X3", "X1, X3, X5",
"X1, X2, X3, X5", "X1, X2, X3, X4, X5")
colnames(mat2) <- c("cp", "adjr2", "bic", "aic")
kable(mat2)
@ 

\textbf{Remarks:}
\begin{enumerate}
   \item In terms of Mallow's cp, BIC, and AIC, the fourth model seems to be the best.
   \item In terms of adjusted R-squared, model 5 is the better one, although the adjusted
   R-squared is very close for model 4 and 5.
\end{enumerate}

We now use stepwise selection method to evaluate the best possible model as follows.

<<>>=
ols_step_both_aic(m2, details = TRUE)
@ 

\textbf{Remark:}
\begin{enumerate}
   \item The stepwise selection method chooses the model Y on X1, X2, X3, and X5, which is 
   also preferable in terms of the other criteria. Therefore, we select model 4 (i.e., Y on X1, X2, X3, and X5) among all possible
   subsets of model 1 (i.e., Y on X1, X2, X3, X4, and X5).
\end{enumerate}

\subsection{Selecting the Final Model}
We compare the models selected from sections 7.1 and 7.2. I.e., we compare the following models:

Model A: $E(Y) = \gamma_0 + \gamma_1 X_1 + \gamma_2 X_2 + \gamma_3 X_3 + \gamma_6 X_6 $

Model B: $E(Y) = \lambda_0 + \lambda_1 X_1 + \lambda_2 X_2 + \lambda_3 X_3 + \lambda_5 X_5 $

We compare these in terms of adjusted R-squared, AIC, BIC, and Mallow's Cp to choose the better one among these.

<<echo = FALSE>>=
ma <- lm(Y ~ X1 + X2 + X3 + X6, fitdata)
mb <- lm(Y ~ X1 + X2 + X3 + X5, fitdata)

matf <- matrix(c(regsumm1$adjr2[4], AIC(ma), BIC(ma), regsumm1$cp[4],
regsumm2$adjr2[4], AIC(mb), BIC(mb), regsumm2$cp[4]), byrow = TRUE, nrow = 2)
rownames(matf) <- c("Model A", "Model B")
colnames(matf) <- c("adjr2", "aic", "bic", "cp")
kable(matf)
@ 

\subsection*{Conclusion}

\begin{itemize}
   \item From the above table, it is easy to observe that model B is better than model A.
   Therefore, we choose model B (i.e., Y on X1, X2, X3, and X5) as our final model.
\end{itemize}

\clearpage

\section{Final Verification}

As our final model, we take
\begin{equation} 
   E(Y) = \lambda_0 + \lambda_1 X_1 + \lambda_2 X_2 + \lambda_3 X_3 + \lambda_5 X_5
\end{equation}

The following is the summary of the model.

<<echo = FALSE>>=
summary(mb)
@ 

We now check for the influential points.

<<echo = FALSE, fig.height = 5, fig.width = 8>>=
ols_plot_dffits(mb)
ols_plot_cooksd_bar(mb)
@ 

<<echo = FALSE>>=
plot(mb, which = 5)
@ 

\textbf{Remark:}
\begin{enumerate}
   \item We have $ 2^{nd} $, $ 4^{th} $, and $ 10^{th} $ as potential influential points.
   Among these, we only remove the $ 10^{th} $ point, as this differs from the most.
   \item The other two points can be removed, but are not, as this will lead to huge data loss
   for a small dataset like this one.
\end{enumerate}

We thus remove the $ 10^{th} $ point from the dataset altogether and fitted the model (2) again.

<<echo = FALSE>>=
fitdata3 <- fitdata[-c(10), ]
m <- lm(Y ~ X1 + X2 + X3 + X5, fitdata3)
summary(m)
@ 

We perform all the tests for checking whether the assumptions are still holding for this model.

<<echo = FALSE>>=
bptest(m)
shapiro.test(residuals(m))
ols_test_normality(m)[1]
dwtest(m)
@ 

<<>>=
vif(m)
@ 

\textbf{Remark:}
\begin{enumerate}
   \item For model (2), none of the assumptions are violated.
   Therefore, we can indeed consider this as our final model.
\end{enumerate}

The following figure plots the response values (blue) and the predicted values (red).
It serves as a visual indication of the goodness of fit for the final model.

<<echo = FALSE>>=
m_pred <- predict(m)
fitdata4 <- fitdata1[-c(10), ]
xyplot(fitdata4[, "Y"] + m_pred ~ 1:30,
       type = c("g", "b"),
       ylab = "Response and Predicted Values",
       xlab = "Index",
       main = list("Final Regression Model"),
       auto.key = list(columns = 2,
                       text = c("Response", "Predicted")
       )
)
@ 

\clearpage

\section{References and Bibliography}

\begin{enumerate}
   \item Seber, G.A.F., Lee, A.J., \textit{Linear Regression Analysis, Second Edition}, John Wiley and Sons.
   \item Hastie, T., Tibshirani, R., Friedman, J., \textit{The Elements of Statistical Learning, Second Edition}, Springer.
   \item James, G., Witten, D., Hastie, T., Tibshirani, R., \textit{An Introduction to Statistical Learning  with Applications in R}, Springer.
   \item \url{https://www.rdocumentation.org}
   \item \url{https://www.isid.ac.in/~deepayan/R-tutorials/index.html}
\end{enumerate}

\section{Acknowledgements}

We would like to express our deeply felt gratitude and regards towards \textbf{Prof. Dr. Swagata Nandi ma'am}.
Her continuous support, suggestions, and guidance helped us in solving our doubts and figuring out methodologies and procedures.
Her help and vision made it possible for us to finish the project. 

\end{document}